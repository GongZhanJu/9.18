{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9774d2d4-2fa6-40d2-9d2f-f59e23c00eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练数据\n",
    "train_dataset = pd.read_csv('./adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)\n",
    "# 加载测试数据     ！！！ 测试数据集为真实值，不能进行调整，否则将会导致实际模型测试结果和真实预测结果存在偏差，使得最终加工的作品和预期不一致\n",
    "test_dataset = pd.read_csv('./test_adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ff9bac-642c-46ef-a1d5-dcfe243e1261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4158</td>\n",
       "      <td>2.9711</td>\n",
       "      <td>10.7935</td>\n",
       "      <td>7.5279</td>\n",
       "      <td>2.3352</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>3.3367</td>\n",
       "      <td>11.8639</td>\n",
       "      <td>12.7142</td>\n",
       "      <td>...</td>\n",
       "      <td>171.764</td>\n",
       "      <td>1434.24</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>-0.932553</td>\n",
       "      <td>0.285048</td>\n",
       "      <td>-0.1435</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.463969</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.8616</td>\n",
       "      <td>10.1770</td>\n",
       "      <td>7.4684</td>\n",
       "      <td>2.1915</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>2.9661</td>\n",
       "      <td>11.5816</td>\n",
       "      <td>12.2487</td>\n",
       "      <td>...</td>\n",
       "      <td>185.824</td>\n",
       "      <td>1469.19</td>\n",
       "      <td>0.894066</td>\n",
       "      <td>-0.446796</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.689742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9648</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>10.1682</td>\n",
       "      <td>5.9705</td>\n",
       "      <td>2.0629</td>\n",
       "      <td>6.5349</td>\n",
       "      <td>2.8694</td>\n",
       "      <td>3.1185</td>\n",
       "      <td>11.7464</td>\n",
       "      <td>12.2074</td>\n",
       "      <td>...</td>\n",
       "      <td>187.576</td>\n",
       "      <td>1540.76</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.460716</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.992935</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.749580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.6221</td>\n",
       "      <td>10.1487</td>\n",
       "      <td>6.8678</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>6.8806</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>2.3003</td>\n",
       "      <td>11.5545</td>\n",
       "      <td>12.0659</td>\n",
       "      <td>...</td>\n",
       "      <td>189.938</td>\n",
       "      <td>1498.29</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>-0.862448</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.891745</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.984439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3797</td>\n",
       "      <td>1.6852</td>\n",
       "      <td>10.9601</td>\n",
       "      <td>5.0035</td>\n",
       "      <td>3.1659</td>\n",
       "      <td>5.9471</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>2.6402</td>\n",
       "      <td>11.7458</td>\n",
       "      <td>12.7041</td>\n",
       "      <td>...</td>\n",
       "      <td>181.275</td>\n",
       "      <td>1465.11</td>\n",
       "      <td>0.442893</td>\n",
       "      <td>-0.990703</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>-0.506397</td>\n",
       "      <td>-0.997502</td>\n",
       "      <td>0.376126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0     特征1      特征2     特征3     特征4     特征5     特征6     特征7      特征8  \\\n",
       "0  1.4158  2.9711  10.7935  7.5279  2.3352  8.1042  2.3096  3.3367  11.8639   \n",
       "1  0.6280  1.8616  10.1770  7.4684  2.1915  8.5945  0.1379  2.9661  11.5816   \n",
       "2  0.9648  1.8103  10.1682  5.9705  2.0629  6.5349  2.8694  3.1185  11.7464   \n",
       "3  0.7119  1.6221  10.1487  6.8678  2.0694  6.8806  1.5791  2.3003  11.5545   \n",
       "4  0.3797  1.6852  10.9601  5.0035  3.1659  5.9471  0.0858  2.6402  11.7458   \n",
       "\n",
       "       特征9  ...     特征16     特征17       补偿0       补偿1       补偿2     补偿3  \\\n",
       "0  12.7142  ...  171.764  1434.24  0.331511 -0.932553  0.285048 -0.1435   \n",
       "1  12.2487  ...  185.824  1469.19  0.894066 -0.446796  0.058519 -0.4624   \n",
       "2  12.2074  ...  187.576  1540.76  0.999982  0.460716  0.997809 -0.4624   \n",
       "3  12.0659  ...  189.938  1498.29  0.998794 -0.862448  0.329694 -0.4624   \n",
       "4  12.7041  ...  181.275  1465.11  0.442893 -0.990703 -0.151400  0.2245   \n",
       "\n",
       "        补偿4       补偿5       补偿6       补偿7  \n",
       "0 -0.833982  0.767568  0.463969  1.904800  \n",
       "1  0.715252  0.999105  0.988844  0.689742  \n",
       "2  0.723031  0.992935  0.682903  0.749580  \n",
       "3  0.891745  0.015078  0.997127  0.984439  \n",
       "4  0.617880 -0.506397 -0.997502  0.376126  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "dataset = train_dataset.copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf23b092-bf2a-4921-a67d-b183f589d185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n",
      "(2998, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.008837</td>\n",
       "      <td>2.018090</td>\n",
       "      <td>10.501365</td>\n",
       "      <td>6.980399</td>\n",
       "      <td>2.991379</td>\n",
       "      <td>6.986831</td>\n",
       "      <td>1.983407</td>\n",
       "      <td>2.994988</td>\n",
       "      <td>11.739018</td>\n",
       "      <td>12.600981</td>\n",
       "      <td>...</td>\n",
       "      <td>172.242767</td>\n",
       "      <td>1474.643939</td>\n",
       "      <td>0.525279</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.250365</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>0.524640</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.561442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.286296</td>\n",
       "      <td>1.134771</td>\n",
       "      <td>0.574884</td>\n",
       "      <td>1.142659</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>...</td>\n",
       "      <td>12.198489</td>\n",
       "      <td>54.894522</td>\n",
       "      <td>0.399656</td>\n",
       "      <td>0.699439</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.672572</td>\n",
       "      <td>0.494762</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>1.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000300</td>\n",
       "      <td>5.000700</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>5.000800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>11.155400</td>\n",
       "      <td>11.750600</td>\n",
       "      <td>...</td>\n",
       "      <td>121.207000</td>\n",
       "      <td>1377.980000</td>\n",
       "      <td>-0.950279</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.240660</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.863171</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-3.354270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.518975</td>\n",
       "      <td>1.506825</td>\n",
       "      <td>10.257700</td>\n",
       "      <td>6.033325</td>\n",
       "      <td>2.501750</td>\n",
       "      <td>6.004525</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>2.509325</td>\n",
       "      <td>11.668100</td>\n",
       "      <td>12.485025</td>\n",
       "      <td>...</td>\n",
       "      <td>165.299500</td>\n",
       "      <td>1437.355000</td>\n",
       "      <td>0.279705</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>-0.096690</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.847946</td>\n",
       "      <td>0.307439</td>\n",
       "      <td>-0.492667</td>\n",
       "      <td>-0.107755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.042550</td>\n",
       "      <td>10.497350</td>\n",
       "      <td>6.962800</td>\n",
       "      <td>2.955600</td>\n",
       "      <td>6.956750</td>\n",
       "      <td>1.985950</td>\n",
       "      <td>2.986350</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>12.644450</td>\n",
       "      <td>...</td>\n",
       "      <td>174.141000</td>\n",
       "      <td>1456.825000</td>\n",
       "      <td>0.617192</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.550868</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.495988</td>\n",
       "      <td>0.390784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.488325</td>\n",
       "      <td>2.523625</td>\n",
       "      <td>10.750450</td>\n",
       "      <td>7.944000</td>\n",
       "      <td>3.481775</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>3.496425</td>\n",
       "      <td>11.835725</td>\n",
       "      <td>12.747475</td>\n",
       "      <td>...</td>\n",
       "      <td>181.637250</td>\n",
       "      <td>1496.542500</td>\n",
       "      <td>0.860046</td>\n",
       "      <td>0.696679</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>1.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.998200</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>10.999300</td>\n",
       "      <td>8.996300</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>8.998500</td>\n",
       "      <td>3.996200</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>11.950100</td>\n",
       "      <td>12.933200</td>\n",
       "      <td>...</td>\n",
       "      <td>193.145000</td>\n",
       "      <td>1785.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.832180</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.585870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               特征0          特征1          特征2          特征3          特征4  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean      1.008837     2.018090    10.501365     6.980399     2.991379   \n",
       "std       0.573132     0.578950     0.286296     1.134771     0.574884   \n",
       "min       0.000500     1.000000    10.000300     5.000700     2.000400   \n",
       "25%       0.518975     1.506825    10.257700     6.033325     2.501750   \n",
       "50%       1.014300     2.042550    10.497350     6.962800     2.955600   \n",
       "75%       1.488325     2.523625    10.750450     7.944000     3.481775   \n",
       "max       1.998200     2.998600    10.999300     8.996300     3.999400   \n",
       "\n",
       "               特征5          特征6          特征7          特征8          特征9  ...  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  ...   \n",
       "mean      6.986831     1.983407     2.994988    11.739018    12.600981  ...   \n",
       "std       1.142659     1.144038     0.577503     0.128526     0.196063  ...   \n",
       "min       5.000800     0.000100     2.000700    11.155400    11.750600  ...   \n",
       "25%       6.004525     1.001925     2.509325    11.668100    12.485025  ...   \n",
       "50%       6.956750     1.985950     2.986350    11.762100    12.644450  ...   \n",
       "75%       7.957600     2.953800     3.496425    11.835725    12.747475  ...   \n",
       "max       8.998500     3.996200     3.999400    11.950100    12.933200  ...   \n",
       "\n",
       "              特征16         特征17          补偿0          补偿1          补偿2  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean    172.242767  1474.643939     0.525279     0.005173     0.250365   \n",
       "std      12.198489    54.894522     0.399656     0.699439     0.497970   \n",
       "min     121.207000  1377.980000    -0.950279    -0.999992    -1.240660   \n",
       "25%     165.299500  1437.355000     0.279705    -0.693314    -0.096690   \n",
       "50%     174.141000  1456.825000     0.617192     0.015394     0.250205   \n",
       "75%     181.637250  1496.542500     0.860046     0.696679     0.604809   \n",
       "max     193.145000  1785.880000     1.000000     1.000000     1.832180   \n",
       "\n",
       "               补偿3          补偿4          补偿5          补偿6          补偿7  \n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  \n",
       "mean      0.010439    -0.269551     0.524640     0.222959     0.561442  \n",
       "std       0.330564     0.672572     0.494762     0.716798     1.124138  \n",
       "min      -0.533200    -1.000000    -0.863171    -0.999998    -3.354270  \n",
       "25%      -0.143500    -0.847946     0.307439    -0.492667    -0.107755  \n",
       "50%      -0.143500    -0.550868     0.709205     0.495988     0.390784  \n",
       "75%       0.224500     0.370577     0.905374     0.877369     1.159790  \n",
       "max       0.632400     0.999997     0.999997     1.000000     6.585870  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds=ds.drop_duplicates(w)\n",
    "print(ds.shape)\n",
    "ds.describe()\n",
    "\n",
    "ds1=test_dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds1=ds1.drop_duplicates(w)\n",
    "print(ds1.shape)\n",
    "ds1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9880db93-b4a2-4d05-b31f-2170c37e4c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 [   1.       1.998   10.5      6.998    3.001    6.995    2.005    2.999\n",
      "   11.736   12.604    1.768    0.649   19.417   19.598    3.349   17.983\n",
      "  172.057 1475.054]\n",
      "方差 [   0.332    0.332    0.083    1.327    0.333    1.337    1.331    0.336\n",
      "    0.017    0.039    0.074    0.213    0.163    0.069   10.31   525.861\n",
      "  149.833 3136.668]\n"
     ]
    }
   ],
   "source": [
    "average = np.average(ds.values[:,:18], axis=0)\n",
    "variance = np.var(ds.values[:,:18], axis=0)\n",
    "print('均值', average)\n",
    "print('方差', variance)\n",
    "#\n",
    "#{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639,\n",
    "#             12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
    "\n",
    "#{'outputs': [[0.31156069, -0.752204835, 0.293267965, -0.0997265279, -0.819844842, 0.861867249,\n",
    "#              0.451463282, 1.86217737]]}\n",
    "\n",
    "#{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
    "#y1 MSE:0.0057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b9fe5dc-c75c-4487-8662-e50c62cd34e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #计算前18列特征的均值和标准差\n",
    "# # mean = np.mean(ds.values[:, :18], axis=0)\n",
    "# # std = np.std(ds.values[:, :18], axis=0)\n",
    "\n",
    "# mean = np.mean(test_dataset.values[:, :18], axis=0)\n",
    "# std = np.std(test_dataset.values[:, :18], axis=0)\n",
    "# # mean1 = np.mean(ds1.values[:, :18], axis=0)\n",
    "# # std1 = np.std(ds1.values[:, :18], axis=0)\n",
    "# # 定义上限和下限\n",
    "# upper_limit = mean + 3 * std\n",
    "# lower_limit = mean - 3 * std\n",
    "\n",
    "\n",
    "\n",
    "# # upper_limit1 = mean1 + 3 * std\n",
    "# # lower_limit1 = mean1 - 3 * std\n",
    "# # 使用布尔索引删除超出上限和下限的行\n",
    "# cleaned_ds = ds[~((ds.values[:, :18] > upper_limit) | (ds.values[:, :18] < lower_limit)).any(axis=1)]\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['补偿7']>-2)].sample(frac=0.2,random_state=0).index)\n",
    "\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['补偿3']>-0.5)].sample(frac=0.2,random_state=0).index)\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征9']>12)].sample(frac=0.2,random_state=0).index)\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征10']>1.75)].sample(frac=0.2,random_state=0).index)\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征12']>18)].sample(frac=0.2,random_state=0).index)\n",
    "# # cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征13']>18.75)].sample(frac=0.2,random_state=0).index)\n",
    "# # cleaned_ds1 = ds1[~((ds1.values[:, :18] > upper_limit1) | (ds1.values[:, :18] < lower_limit1)).any(axis=1)]\n",
    "\n",
    "# # 打印清理后数据集的形状\n",
    "# print(cleaned_ds.shape)\n",
    "\n",
    "# print(cleaned_ds.shape)\n",
    "# cleaned_ds.describe()\n",
    "\n",
    "# cleaned_ds=pd.DataFrame(cleaned_ds)\n",
    "# variance=np.var(cleaned_ds)\n",
    "\n",
    "# print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3eeff0-7786-4933-ae06-3de78efaaa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 18) (50000, 8)\n",
      "(2500, 18) (2500, 8)\n",
      "(2998, 18) (2998, 8)\n"
     ]
    }
   ],
   "source": [
    "train_ds=dataset.sample(frac=1,random_state=0)\n",
    "# val_ds=dataset.drop(train_ds.index)\n",
    "val_ds=dataset.sample(frac=0.05,random_state=0)\n",
    "\n",
    "# train_ds=cleaned_ds.sample(frac=0.9,random_state=0)\n",
    "# val_ds=cleaned_ds.drop(train_ds.index)\n",
    "\n",
    "#训练集\n",
    "train_features=train_ds.values[:,:18]\n",
    "train_labels=train_ds.values[:,18:]\n",
    "\n",
    "#验证集\n",
    "val_features=val_ds.values[:,:18]\n",
    "val_labels=val_ds.values[:,18:]\n",
    "\n",
    "# 测试集\n",
    "test_features=test_dataset.values[:,:18]\n",
    "test_labels=test_dataset.values[:,18:]\n",
    "\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(val_features.shape,val_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3c8e7b0-992e-4600-b098-f4fcfd4c23c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "976c17ce-944d-4bb6-b843-246da048f49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------构建模型及训练-----------------\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d9269ea-8503-41b3-b510-090df5d2777a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               5700      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 2408      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 369,345\n",
      "Trainable params: 369,308\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "tf.keras.utils.get_custom_objects()[\"mish\"]=mish\n",
    "# 创建Normalizaiton层\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "# # 计算并设置归一化参数\n",
    "#normalizer.adapt(train_features)\n",
    "# 构建模型\n",
    "normalizer.adapt(val_features)\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,  # 归一化层作为第一层\n",
    "    layers.Dense(300, activation=\"gelu\", input_dim=train_features.shape[1]),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    # layers.Dense(300, activation=\"mish\"),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(train_labels.shape[1])\n",
    "    # normalizer,  # 归一化层作为第一层\n",
    "    # layers.Dense(100, activation=\"relu\"),\n",
    "    # layers.Dense(train_labels.shape[1])\n",
    "])\n",
    "\n",
    "# optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")  # 根据情况调整参数\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0fb0d64-f9e3-4191-91e3-334ee0d3cdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "391/391 [==============================] - 3s 5ms/step - loss: 0.1523 - val_loss: 0.1302 - lr: 0.0010\n",
      "Epoch 2/400\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1202 - val_loss: 0.1124 - lr: 0.0010\n",
      "Epoch 3/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1107 - val_loss: 0.1061 - lr: 0.0010\n",
      "Epoch 4/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1065 - val_loss: 0.1011 - lr: 0.0010\n",
      "Epoch 5/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1040 - val_loss: 0.1001 - lr: 0.0010\n",
      "Epoch 6/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1029 - val_loss: 0.1012 - lr: 0.0010\n",
      "Epoch 7/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1018 - val_loss: 0.0995 - lr: 0.0010\n",
      "Epoch 8/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1010 - val_loss: 0.0983 - lr: 0.0010\n",
      "Epoch 9/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1001 - val_loss: 0.0993 - lr: 0.0010\n",
      "Epoch 10/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0996 - val_loss: 0.0981 - lr: 0.0010\n",
      "Epoch 11/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0994 - val_loss: 0.0963 - lr: 0.0010\n",
      "Epoch 12/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0985 - val_loss: 0.0959 - lr: 0.0010\n",
      "Epoch 13/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0987 - val_loss: 0.0970 - lr: 0.0010\n",
      "Epoch 14/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0978 - val_loss: 0.0963 - lr: 0.0010\n",
      "Epoch 15/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0974 - val_loss: 0.0961 - lr: 0.0010\n",
      "Epoch 16/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0973 - val_loss: 0.0945 - lr: 0.0010\n",
      "Epoch 17/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0963 - val_loss: 0.0929 - lr: 0.0010\n",
      "Epoch 18/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0962 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 19/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0956 - val_loss: 0.0928 - lr: 0.0010\n",
      "Epoch 20/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0950 - val_loss: 0.0915 - lr: 0.0010\n",
      "Epoch 21/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0945 - val_loss: 0.0901 - lr: 0.0010\n",
      "Epoch 22/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0934 - val_loss: 0.0895 - lr: 0.0010\n",
      "Epoch 23/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0928 - val_loss: 0.0885 - lr: 0.0010\n",
      "Epoch 24/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0919 - val_loss: 0.0870 - lr: 0.0010\n",
      "Epoch 25/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0903 - val_loss: 0.0871 - lr: 0.0010\n",
      "Epoch 26/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0889 - val_loss: 0.0860 - lr: 0.0010\n",
      "Epoch 27/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0872 - val_loss: 0.0814 - lr: 0.0010\n",
      "Epoch 28/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0853 - val_loss: 0.0814 - lr: 0.0010\n",
      "Epoch 29/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0825 - val_loss: 0.0769 - lr: 0.0010\n",
      "Epoch 30/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0806 - val_loss: 0.0741 - lr: 0.0010\n",
      "Epoch 31/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0776 - val_loss: 0.0725 - lr: 0.0010\n",
      "Epoch 32/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0751 - val_loss: 0.0694 - lr: 0.0010\n",
      "Epoch 33/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0720 - val_loss: 0.0647 - lr: 0.0010\n",
      "Epoch 34/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0690 - val_loss: 0.0619 - lr: 0.0010\n",
      "Epoch 35/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0659 - val_loss: 0.0594 - lr: 0.0010\n",
      "Epoch 36/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0630 - val_loss: 0.0558 - lr: 0.0010\n",
      "Epoch 37/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0598 - val_loss: 0.0539 - lr: 0.0010\n",
      "Epoch 38/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0568 - val_loss: 0.0499 - lr: 0.0010\n",
      "Epoch 39/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0541 - val_loss: 0.0461 - lr: 0.0010\n",
      "Epoch 40/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0510 - val_loss: 0.0435 - lr: 0.0010\n",
      "Epoch 41/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0432 - lr: 0.0010\n",
      "Epoch 42/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0458 - val_loss: 0.0405 - lr: 0.0010\n",
      "Epoch 43/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0437 - val_loss: 0.0386 - lr: 0.0010\n",
      "Epoch 44/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0412 - val_loss: 0.0350 - lr: 0.0010\n",
      "Epoch 45/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0389 - val_loss: 0.0336 - lr: 0.0010\n",
      "Epoch 46/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0370 - val_loss: 0.0314 - lr: 0.0010\n",
      "Epoch 47/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0354 - val_loss: 0.0300 - lr: 0.0010\n",
      "Epoch 48/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0331 - val_loss: 0.0296 - lr: 0.0010\n",
      "Epoch 49/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0321 - val_loss: 0.0278 - lr: 0.0010\n",
      "Epoch 50/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0305 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 51/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0293 - val_loss: 0.0248 - lr: 0.0010\n",
      "Epoch 52/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0278 - val_loss: 0.0241 - lr: 0.0010\n",
      "Epoch 53/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0262 - val_loss: 0.0241 - lr: 0.0010\n",
      "Epoch 54/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0253 - val_loss: 0.0232 - lr: 0.0010\n",
      "Epoch 55/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0247 - val_loss: 0.0224 - lr: 0.0010\n",
      "Epoch 56/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0241 - val_loss: 0.0221 - lr: 0.0010\n",
      "Epoch 57/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0227 - val_loss: 0.0193 - lr: 0.0010\n",
      "Epoch 58/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0218 - val_loss: 0.0195 - lr: 0.0010\n",
      "Epoch 59/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0208 - val_loss: 0.0184 - lr: 0.0010\n",
      "Epoch 60/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0204 - val_loss: 0.0188 - lr: 0.0010\n",
      "Epoch 61/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0201 - val_loss: 0.0188 - lr: 0.0010\n",
      "Epoch 62/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0193 - val_loss: 0.0170 - lr: 0.0010\n",
      "Epoch 63/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0187 - val_loss: 0.0173 - lr: 0.0010\n",
      "Epoch 64/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0182 - val_loss: 0.0175 - lr: 0.0010\n",
      "Epoch 65/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0177 - val_loss: 0.0167 - lr: 0.0010\n",
      "Epoch 66/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0173 - val_loss: 0.0159 - lr: 0.0010\n",
      "Epoch 67/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0168 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 68/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0166 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 69/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0162 - val_loss: 0.0147 - lr: 0.0010\n",
      "Epoch 70/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0157 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 71/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0157 - val_loss: 0.0149 - lr: 0.0010\n",
      "Epoch 72/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0151 - val_loss: 0.0152 - lr: 0.0010\n",
      "Epoch 73/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0148 - val_loss: 0.0140 - lr: 0.0010\n",
      "Epoch 74/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0146 - val_loss: 0.0145 - lr: 0.0010\n",
      "Epoch 75/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0143 - val_loss: 0.0139 - lr: 0.0010\n",
      "Epoch 76/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0139 - val_loss: 0.0140 - lr: 0.0010\n",
      "Epoch 77/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0138 - val_loss: 0.0134 - lr: 0.0010\n",
      "Epoch 78/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0138 - val_loss: 0.0127 - lr: 0.0010\n",
      "Epoch 79/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0134 - val_loss: 0.0132 - lr: 0.0010\n",
      "Epoch 80/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0131 - val_loss: 0.0127 - lr: 0.0010\n",
      "Epoch 81/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0128 - val_loss: 0.0124 - lr: 0.0010\n",
      "Epoch 82/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0131 - val_loss: 0.0126 - lr: 0.0010\n",
      "Epoch 83/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0126 - val_loss: 0.0119 - lr: 0.0010\n",
      "Epoch 84/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0125 - val_loss: 0.0126 - lr: 0.0010\n",
      "Epoch 85/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0124 - val_loss: 0.0121 - lr: 0.0010\n",
      "Epoch 86/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0121 - val_loss: 0.0121 - lr: 0.0010\n",
      "Epoch 87/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0120 - val_loss: 0.0115 - lr: 0.0010\n",
      "Epoch 88/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0116 - val_loss: 0.0113 - lr: 0.0010\n",
      "Epoch 89/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0117 - val_loss: 0.0109 - lr: 0.0010\n",
      "Epoch 90/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0116 - val_loss: 0.0110 - lr: 0.0010\n",
      "Epoch 91/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0115 - val_loss: 0.0111 - lr: 0.0010\n",
      "Epoch 92/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0114 - val_loss: 0.0113 - lr: 0.0010\n",
      "Epoch 93/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0110 - val_loss: 0.0106 - lr: 0.0010\n",
      "Epoch 94/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0112 - val_loss: 0.0106 - lr: 0.0010\n",
      "Epoch 95/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0108 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 96/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0106 - val_loss: 0.0103 - lr: 0.0010\n",
      "Epoch 97/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0107 - val_loss: 0.0104 - lr: 0.0010\n",
      "Epoch 98/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0105 - val_loss: 0.0105 - lr: 0.0010\n",
      "Epoch 99/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0106 - val_loss: 0.0104 - lr: 0.0010\n",
      "Epoch 100/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0103 - val_loss: 0.0098 - lr: 0.0010\n",
      "Epoch 101/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0100 - val_loss: 0.0100 - lr: 0.0010\n",
      "Epoch 102/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0103 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 103/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0102 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 104/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0102 - val_loss: 0.0103 - lr: 0.0010\n",
      "Epoch 105/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0102 - val_loss: 0.0101 - lr: 0.0010\n",
      "Epoch 106/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0083 - val_loss: 0.0068 - lr: 9.0000e-04\n",
      "Epoch 107/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0069 - val_loss: 0.0068 - lr: 9.0000e-04\n",
      "Epoch 108/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0070 - val_loss: 0.0072 - lr: 9.0000e-04\n",
      "Epoch 109/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0076 - val_loss: 0.0079 - lr: 9.0000e-04\n",
      "Epoch 110/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0083 - val_loss: 0.0084 - lr: 9.0000e-04\n",
      "Epoch 111/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0084 - val_loss: 0.0081 - lr: 9.0000e-04\n",
      "Epoch 112/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0069 - val_loss: 0.0057 - lr: 8.1000e-04\n",
      "Epoch 113/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0057 - val_loss: 0.0055 - lr: 8.1000e-04\n",
      "Epoch 114/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0057 - val_loss: 0.0059 - lr: 8.1000e-04\n",
      "Epoch 115/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0062 - val_loss: 0.0065 - lr: 8.1000e-04\n",
      "Epoch 116/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0064 - val_loss: 0.0063 - lr: 8.1000e-04\n",
      "Epoch 117/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0069 - val_loss: 0.0066 - lr: 8.1000e-04\n",
      "Epoch 118/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0069 - val_loss: 0.0064 - lr: 8.1000e-04\n",
      "Epoch 119/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0056 - val_loss: 0.0048 - lr: 7.2900e-04\n",
      "Epoch 120/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0046 - lr: 7.2900e-04\n",
      "Epoch 121/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0045 - val_loss: 0.0050 - lr: 7.2900e-04\n",
      "Epoch 122/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0050 - val_loss: 0.0053 - lr: 7.2900e-04\n",
      "Epoch 123/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0058 - val_loss: 0.0056 - lr: 7.2900e-04\n",
      "Epoch 124/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0057 - val_loss: 0.0054 - lr: 7.2900e-04\n",
      "Epoch 125/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0052 - val_loss: 0.0053 - lr: 7.2900e-04\n",
      "Epoch 126/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0044 - val_loss: 0.0036 - lr: 6.5610e-04\n",
      "Epoch 127/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0037 - val_loss: 0.0038 - lr: 6.5610e-04\n",
      "Epoch 128/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0037 - val_loss: 0.0040 - lr: 6.5610e-04\n",
      "Epoch 129/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0043 - val_loss: 0.0044 - lr: 6.5610e-04\n",
      "Epoch 130/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0045 - val_loss: 0.0043 - lr: 6.5610e-04\n",
      "Epoch 131/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0044 - val_loss: 0.0044 - lr: 6.5610e-04\n",
      "Epoch 132/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0037 - val_loss: 0.0030 - lr: 5.9049e-04\n",
      "Epoch 133/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0031 - val_loss: 0.0030 - lr: 5.9049e-04\n",
      "Epoch 134/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0031 - val_loss: 0.0031 - lr: 5.9049e-04\n",
      "Epoch 135/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0035 - val_loss: 0.0037 - lr: 5.9049e-04\n",
      "Epoch 136/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0038 - val_loss: 0.0036 - lr: 5.9049e-04\n",
      "Epoch 137/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0037 - val_loss: 0.0036 - lr: 5.9049e-04\n",
      "Epoch 138/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0031 - val_loss: 0.0027 - lr: 5.3144e-04\n",
      "Epoch 139/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0025 - val_loss: 0.0024 - lr: 5.3144e-04\n",
      "Epoch 140/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0026 - val_loss: 0.0030 - lr: 5.3144e-04\n",
      "Epoch 141/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0028 - val_loss: 0.0029 - lr: 5.3144e-04\n",
      "Epoch 142/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0029 - val_loss: 0.0028 - lr: 5.3144e-04\n",
      "Epoch 143/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0030 - val_loss: 0.0030 - lr: 5.3144e-04\n",
      "Epoch 144/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0031 - val_loss: 0.0031 - lr: 5.3144e-04\n",
      "Epoch 145/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0026 - val_loss: 0.0022 - lr: 4.7830e-04\n",
      "Epoch 146/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0021 - val_loss: 0.0021 - lr: 4.7830e-04\n",
      "Epoch 147/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0021 - val_loss: 0.0021 - lr: 4.7830e-04\n",
      "Epoch 148/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0022 - val_loss: 0.0023 - lr: 4.7830e-04\n",
      "Epoch 149/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0025 - val_loss: 0.0027 - lr: 4.7830e-04\n",
      "Epoch 150/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0027 - val_loss: 0.0026 - lr: 4.7830e-04\n",
      "Epoch 151/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0022 - val_loss: 0.0019 - lr: 4.3047e-04\n",
      "Epoch 152/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0017 - val_loss: 0.0016 - lr: 4.3047e-04\n",
      "Epoch 153/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 4.3047e-04\n",
      "Epoch 154/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 4.3047e-04\n",
      "Epoch 155/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0021 - val_loss: 0.0021 - lr: 4.3047e-04\n",
      "Epoch 156/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0021 - val_loss: 0.0020 - lr: 4.3047e-04\n",
      "Epoch 157/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0021 - val_loss: 0.0021 - lr: 4.3047e-04\n",
      "Epoch 158/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0018 - val_loss: 0.0016 - lr: 3.8742e-04\n",
      "Epoch 159/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0014 - val_loss: 0.0013 - lr: 3.8742e-04\n",
      "Epoch 160/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0014 - val_loss: 0.0015 - lr: 3.8742e-04\n",
      "Epoch 161/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0016 - val_loss: 0.0017 - lr: 3.8742e-04\n",
      "Epoch 162/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0019 - val_loss: 0.0018 - lr: 3.8742e-04\n",
      "Epoch 163/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0017 - val_loss: 0.0017 - lr: 3.8742e-04\n",
      "Epoch 164/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0017 - val_loss: 0.0016 - lr: 3.8742e-04\n",
      "Epoch 165/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0014 - val_loss: 0.0013 - lr: 3.4868e-04\n",
      "Epoch 166/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 3.4868e-04\n",
      "Epoch 167/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 3.4868e-04\n",
      "Epoch 168/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 3.4868e-04\n",
      "Epoch 169/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0014 - val_loss: 0.0015 - lr: 3.4868e-04\n",
      "Epoch 170/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0015 - val_loss: 0.0015 - lr: 3.4868e-04\n",
      "Epoch 171/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0015 - val_loss: 0.0015 - lr: 3.4868e-04\n",
      "Epoch 172/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0011 - lr: 3.1381e-04\n",
      "Epoch 173/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 9.8514e-04 - lr: 3.1381e-04\n",
      "Epoch 174/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.9787e-04 - val_loss: 0.0010 - lr: 3.1381e-04\n",
      "Epoch 175/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 3.1381e-04\n",
      "Epoch 176/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 3.1381e-04\n",
      "Epoch 177/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0012 - lr: 3.1381e-04\n",
      "Epoch 178/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 9.6167e-04 - lr: 2.8243e-04\n",
      "Epoch 179/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.1101e-04 - val_loss: 8.5986e-04 - lr: 2.8243e-04\n",
      "Epoch 180/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.6555e-04 - val_loss: 8.6024e-04 - lr: 2.8243e-04\n",
      "Epoch 181/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.2674e-04 - val_loss: 9.2624e-04 - lr: 2.8243e-04\n",
      "Epoch 182/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 2.8243e-04\n",
      "Epoch 183/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 0.0010 - lr: 2.8243e-04\n",
      "Epoch 184/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 9.7594e-04 - lr: 2.8243e-04\n",
      "Epoch 185/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.7801e-04 - val_loss: 7.7235e-04 - lr: 2.5419e-04\n",
      "Epoch 186/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.4737e-04 - val_loss: 7.3649e-04 - lr: 2.5419e-04\n",
      "Epoch 187/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.4116e-04 - val_loss: 7.3877e-04 - lr: 2.5419e-04\n",
      "Epoch 188/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.8443e-04 - val_loss: 7.7837e-04 - lr: 2.5419e-04\n",
      "Epoch 189/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.5249e-04 - val_loss: 9.0926e-04 - lr: 2.5419e-04\n",
      "Epoch 190/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.0214e-04 - val_loss: 8.3366e-04 - lr: 2.5419e-04\n",
      "Epoch 191/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.0933e-04 - val_loss: 9.4720e-04 - lr: 2.5419e-04\n",
      "Epoch 192/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.8083e-04 - val_loss: 6.6879e-04 - lr: 2.2877e-04\n",
      "Epoch 193/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.5395e-04 - val_loss: 6.6110e-04 - lr: 2.2877e-04\n",
      "Epoch 194/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.4181e-04 - val_loss: 6.2415e-04 - lr: 2.2877e-04\n",
      "Epoch 195/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.6693e-04 - val_loss: 6.9888e-04 - lr: 2.2877e-04\n",
      "Epoch 196/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.1191e-04 - val_loss: 7.2435e-04 - lr: 2.2877e-04\n",
      "Epoch 197/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.4580e-04 - val_loss: 7.3971e-04 - lr: 2.2877e-04\n",
      "Epoch 198/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.5738e-04 - val_loss: 7.4724e-04 - lr: 2.2877e-04\n",
      "Epoch 199/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.4751e-04 - val_loss: 7.1865e-04 - lr: 2.2877e-04\n",
      "Epoch 200/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.5028e-04 - val_loss: 5.8342e-04 - lr: 2.0589e-04\n",
      "Epoch 201/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.6397e-04 - val_loss: 5.2705e-04 - lr: 2.0589e-04\n",
      "Epoch 202/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.4579e-04 - val_loss: 5.4340e-04 - lr: 2.0589e-04\n",
      "Epoch 203/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.7508e-04 - val_loss: 6.0199e-04 - lr: 2.0589e-04\n",
      "Epoch 204/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.1806e-04 - val_loss: 6.4159e-04 - lr: 2.0589e-04\n",
      "Epoch 205/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.7540e-04 - val_loss: 5.2280e-04 - lr: 1.8530e-04\n",
      "Epoch 206/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.0813e-04 - val_loss: 5.0474e-04 - lr: 1.8530e-04\n",
      "Epoch 207/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.9967e-04 - val_loss: 5.2314e-04 - lr: 1.8530e-04\n",
      "Epoch 208/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.2419e-04 - val_loss: 4.9960e-04 - lr: 1.8530e-04\n",
      "Epoch 209/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.3956e-04 - val_loss: 5.1620e-04 - lr: 1.8530e-04\n",
      "Epoch 210/400\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.4918e-04 - val_loss: 5.4389e-04 - lr: 1.8530e-04\n",
      "Epoch 211/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.9571e-04 - val_loss: 4.4836e-04 - lr: 1.6677e-04\n",
      "Epoch 212/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.4034e-04 - val_loss: 4.2397e-04 - lr: 1.6677e-04\n",
      "Epoch 213/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.3293e-04 - val_loss: 4.3093e-04 - lr: 1.6677e-04\n",
      "Epoch 214/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.4990e-04 - val_loss: 4.3350e-04 - lr: 1.6677e-04\n",
      "Epoch 215/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.6871e-04 - val_loss: 4.7940e-04 - lr: 1.6677e-04\n",
      "Epoch 216/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.3534e-04 - val_loss: 3.8665e-04 - lr: 1.5009e-04\n",
      "Epoch 217/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.9474e-04 - val_loss: 3.7479e-04 - lr: 1.5009e-04\n",
      "Epoch 218/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.8619e-04 - val_loss: 3.7951e-04 - lr: 1.5009e-04\n",
      "Epoch 219/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.9616e-04 - val_loss: 4.1177e-04 - lr: 1.5009e-04\n",
      "Epoch 220/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.2168e-04 - val_loss: 4.1100e-04 - lr: 1.5009e-04\n",
      "Epoch 221/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.2641e-04 - val_loss: 4.0853e-04 - lr: 1.5009e-04\n",
      "Epoch 222/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.8504e-04 - val_loss: 3.4889e-04 - lr: 1.3509e-04\n",
      "Epoch 223/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.4580e-04 - val_loss: 3.3558e-04 - lr: 1.3509e-04\n",
      "Epoch 224/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.3854e-04 - val_loss: 3.3199e-04 - lr: 1.3509e-04\n",
      "Epoch 225/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.5100e-04 - val_loss: 3.5518e-04 - lr: 1.3509e-04\n",
      "Epoch 226/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.6869e-04 - val_loss: 3.6320e-04 - lr: 1.3509e-04\n",
      "Epoch 227/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.4371e-04 - val_loss: 3.1413e-04 - lr: 1.2158e-04\n",
      "Epoch 228/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.1545e-04 - val_loss: 2.9969e-04 - lr: 1.2158e-04\n",
      "Epoch 229/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.1013e-04 - val_loss: 3.0271e-04 - lr: 1.2158e-04\n",
      "Epoch 230/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.1333e-04 - val_loss: 3.1137e-04 - lr: 1.2158e-04\n",
      "Epoch 231/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.2461e-04 - val_loss: 3.1797e-04 - lr: 1.2158e-04\n",
      "Epoch 232/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.0844e-04 - val_loss: 2.8557e-04 - lr: 1.0942e-04\n",
      "Epoch 233/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8600e-04 - val_loss: 2.7189e-04 - lr: 1.0942e-04\n",
      "Epoch 234/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8229e-04 - val_loss: 2.7734e-04 - lr: 1.0942e-04\n",
      "Epoch 235/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8770e-04 - val_loss: 2.8967e-04 - lr: 1.0942e-04\n",
      "Epoch 236/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.9587e-04 - val_loss: 2.8561e-04 - lr: 1.0942e-04\n",
      "Epoch 237/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.9714e-04 - val_loss: 2.9105e-04 - lr: 1.0942e-04\n",
      "Epoch 238/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.7845e-04 - val_loss: 2.5905e-04 - lr: 9.8477e-05\n",
      "Epoch 239/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5819e-04 - val_loss: 2.5188e-04 - lr: 9.8477e-05\n",
      "Epoch 240/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5475e-04 - val_loss: 2.4835e-04 - lr: 9.8477e-05\n",
      "Epoch 241/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5960e-04 - val_loss: 2.5457e-04 - lr: 9.8477e-05\n",
      "Epoch 242/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.6553e-04 - val_loss: 2.6222e-04 - lr: 9.8477e-05\n",
      "Epoch 243/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5352e-04 - val_loss: 2.3851e-04 - lr: 8.8629e-05\n",
      "Epoch 244/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.3919e-04 - val_loss: 2.3233e-04 - lr: 8.8629e-05\n",
      "Epoch 245/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.3547e-04 - val_loss: 2.3440e-04 - lr: 8.8629e-05\n",
      "Epoch 246/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.3694e-04 - val_loss: 2.3563e-04 - lr: 8.8629e-05\n",
      "Epoch 247/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.4565e-04 - val_loss: 2.3616e-04 - lr: 8.8629e-05\n",
      "Epoch 248/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.3316e-04 - val_loss: 2.1662e-04 - lr: 7.9766e-05\n",
      "Epoch 249/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.2124e-04 - val_loss: 2.1335e-04 - lr: 7.9766e-05\n",
      "Epoch 250/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.1892e-04 - val_loss: 2.1353e-04 - lr: 7.9766e-05\n",
      "Epoch 251/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.2086e-04 - val_loss: 2.1633e-04 - lr: 7.9766e-05\n",
      "Epoch 252/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.2405e-04 - val_loss: 2.1383e-04 - lr: 7.9766e-05\n",
      "Epoch 253/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.1327e-04 - val_loss: 2.0085e-04 - lr: 7.1790e-05\n",
      "Epoch 254/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0564e-04 - val_loss: 2.0168e-04 - lr: 7.1790e-05\n",
      "Epoch 255/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0370e-04 - val_loss: 2.0173e-04 - lr: 7.1790e-05\n",
      "Epoch 256/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0569e-04 - val_loss: 2.0298e-04 - lr: 7.1790e-05\n",
      "Epoch 257/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0867e-04 - val_loss: 2.0237e-04 - lr: 7.1790e-05\n",
      "Epoch 258/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9994e-04 - val_loss: 1.9047e-04 - lr: 6.4611e-05\n",
      "Epoch 259/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9200e-04 - val_loss: 1.8487e-04 - lr: 6.4611e-05\n",
      "Epoch 260/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9053e-04 - val_loss: 1.8704e-04 - lr: 6.4611e-05\n",
      "Epoch 261/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9162e-04 - val_loss: 1.8697e-04 - lr: 6.4611e-05\n",
      "Epoch 262/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9353e-04 - val_loss: 1.9022e-04 - lr: 6.4611e-05\n",
      "Epoch 263/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9413e-04 - val_loss: 1.8553e-04 - lr: 6.4611e-05\n",
      "Epoch 264/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9338e-04 - val_loss: 1.8671e-04 - lr: 6.4611e-05\n",
      "Epoch 265/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.8607e-04 - val_loss: 1.7671e-04 - lr: 5.8150e-05\n",
      "Epoch 266/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7892e-04 - val_loss: 1.7666e-04 - lr: 5.8150e-05\n",
      "Epoch 267/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7745e-04 - val_loss: 1.7491e-04 - lr: 5.8150e-05\n",
      "Epoch 268/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7864e-04 - val_loss: 1.7440e-04 - lr: 5.8150e-05\n",
      "Epoch 269/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7981e-04 - val_loss: 1.7517e-04 - lr: 5.8150e-05\n",
      "Epoch 270/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7436e-04 - val_loss: 1.6746e-04 - lr: 5.2335e-05\n",
      "Epoch 271/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6962e-04 - val_loss: 1.6661e-04 - lr: 5.2335e-05\n",
      "Epoch 272/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6897e-04 - val_loss: 1.6379e-04 - lr: 5.2335e-05\n",
      "Epoch 273/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6973e-04 - val_loss: 1.6513e-04 - lr: 5.2335e-05\n",
      "Epoch 274/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7017e-04 - val_loss: 1.6803e-04 - lr: 5.2335e-05\n",
      "Epoch 275/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6533e-04 - val_loss: 1.5997e-04 - lr: 4.7101e-05\n",
      "Epoch 276/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6135e-04 - val_loss: 1.5666e-04 - lr: 4.7101e-05\n",
      "Epoch 277/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6049e-04 - val_loss: 1.5991e-04 - lr: 4.7101e-05\n",
      "Epoch 278/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6101e-04 - val_loss: 1.5854e-04 - lr: 4.7101e-05\n",
      "Epoch 279/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6125e-04 - val_loss: 1.6013e-04 - lr: 4.7101e-05\n",
      "Epoch 280/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5778e-04 - val_loss: 1.5195e-04 - lr: 4.2391e-05\n",
      "Epoch 281/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5424e-04 - val_loss: 1.5139e-04 - lr: 4.2391e-05\n",
      "Epoch 282/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5350e-04 - val_loss: 1.4996e-04 - lr: 4.2391e-05\n",
      "Epoch 283/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5411e-04 - val_loss: 1.5226e-04 - lr: 4.2391e-05\n",
      "Epoch 284/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5362e-04 - val_loss: 1.5109e-04 - lr: 4.2391e-05\n",
      "Epoch 285/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5081e-04 - val_loss: 1.4477e-04 - lr: 3.8152e-05\n",
      "Epoch 286/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4839e-04 - val_loss: 1.4544e-04 - lr: 3.8152e-05\n",
      "Epoch 287/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4743e-04 - val_loss: 1.4401e-04 - lr: 3.8152e-05\n",
      "Epoch 288/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4705e-04 - val_loss: 1.4555e-04 - lr: 3.8152e-05\n",
      "Epoch 289/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4716e-04 - val_loss: 1.4543e-04 - lr: 3.8152e-05\n",
      "Epoch 290/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4439e-04 - val_loss: 1.3997e-04 - lr: 3.4337e-05\n",
      "Epoch 291/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4242e-04 - val_loss: 1.4044e-04 - lr: 3.4337e-05\n",
      "Epoch 292/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4232e-04 - val_loss: 1.4085e-04 - lr: 3.4337e-05\n",
      "Epoch 293/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4182e-04 - val_loss: 1.3893e-04 - lr: 3.4337e-05\n",
      "Epoch 294/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4161e-04 - val_loss: 1.4022e-04 - lr: 3.4337e-05\n",
      "Epoch 295/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3927e-04 - val_loss: 1.3647e-04 - lr: 3.0903e-05\n",
      "Epoch 296/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3755e-04 - val_loss: 1.3523e-04 - lr: 3.0903e-05\n",
      "Epoch 297/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3726e-04 - val_loss: 1.3445e-04 - lr: 3.0903e-05\n",
      "Epoch 298/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3717e-04 - val_loss: 1.3602e-04 - lr: 3.0903e-05\n",
      "Epoch 299/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3731e-04 - val_loss: 1.3490e-04 - lr: 3.0903e-05\n",
      "Epoch 300/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3493e-04 - val_loss: 1.3119e-04 - lr: 2.7813e-05\n",
      "Epoch 301/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3336e-04 - val_loss: 1.3161e-04 - lr: 2.7813e-05\n",
      "Epoch 302/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3274e-04 - val_loss: 1.3160e-04 - lr: 2.7813e-05\n",
      "Epoch 303/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3266e-04 - val_loss: 1.3087e-04 - lr: 2.7813e-05\n",
      "Epoch 304/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3272e-04 - val_loss: 1.3103e-04 - lr: 2.7813e-05\n",
      "Epoch 305/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3075e-04 - val_loss: 1.2843e-04 - lr: 2.5032e-05\n",
      "Epoch 306/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2975e-04 - val_loss: 1.2735e-04 - lr: 2.5032e-05\n",
      "Epoch 307/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2931e-04 - val_loss: 1.2702e-04 - lr: 2.5032e-05\n",
      "Epoch 308/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2915e-04 - val_loss: 1.2756e-04 - lr: 2.5032e-05\n",
      "Epoch 309/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2895e-04 - val_loss: 1.2718e-04 - lr: 2.5032e-05\n",
      "Epoch 310/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2725e-04 - val_loss: 1.2562e-04 - lr: 2.2528e-05\n",
      "Epoch 311/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2615e-04 - val_loss: 1.2509e-04 - lr: 2.2528e-05\n",
      "Epoch 312/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2613e-04 - val_loss: 1.2508e-04 - lr: 2.2528e-05\n",
      "Epoch 313/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2552e-04 - val_loss: 1.2410e-04 - lr: 2.2528e-05\n",
      "Epoch 314/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2543e-04 - val_loss: 1.2328e-04 - lr: 2.2528e-05\n",
      "Epoch 315/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2404e-04 - val_loss: 1.2317e-04 - lr: 2.0276e-05\n",
      "Epoch 316/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2319e-04 - val_loss: 1.2188e-04 - lr: 2.0276e-05\n",
      "Epoch 317/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2303e-04 - val_loss: 1.2192e-04 - lr: 2.0276e-05\n",
      "Epoch 318/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2267e-04 - val_loss: 1.2197e-04 - lr: 2.0276e-05\n",
      "Epoch 319/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2262e-04 - val_loss: 1.2128e-04 - lr: 2.0276e-05\n",
      "Epoch 320/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2132e-04 - val_loss: 1.1952e-04 - lr: 1.8248e-05\n",
      "Epoch 321/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2062e-04 - val_loss: 1.1953e-04 - lr: 1.8248e-05\n",
      "Epoch 322/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2024e-04 - val_loss: 1.1901e-04 - lr: 1.8248e-05\n",
      "Epoch 323/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2015e-04 - val_loss: 1.1900e-04 - lr: 1.8248e-05\n",
      "Epoch 324/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1998e-04 - val_loss: 1.1904e-04 - lr: 1.8248e-05\n",
      "Epoch 325/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1879e-04 - val_loss: 1.1812e-04 - lr: 1.6423e-05\n",
      "Epoch 326/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1815e-04 - val_loss: 1.1789e-04 - lr: 1.6423e-05\n",
      "Epoch 327/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1794e-04 - val_loss: 1.1684e-04 - lr: 1.6423e-05\n",
      "Epoch 328/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1781e-04 - val_loss: 1.1670e-04 - lr: 1.6423e-05\n",
      "Epoch 329/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1748e-04 - val_loss: 1.1666e-04 - lr: 1.6423e-05\n",
      "Epoch 330/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1654e-04 - val_loss: 1.1628e-04 - lr: 1.4781e-05\n",
      "Epoch 331/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1606e-04 - val_loss: 1.1557e-04 - lr: 1.4781e-05\n",
      "Epoch 332/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1580e-04 - val_loss: 1.1506e-04 - lr: 1.4781e-05\n",
      "Epoch 333/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1558e-04 - val_loss: 1.1501e-04 - lr: 1.4781e-05\n",
      "Epoch 334/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1547e-04 - val_loss: 1.1523e-04 - lr: 1.4781e-05\n",
      "Epoch 335/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1454e-04 - val_loss: 1.1420e-04 - lr: 1.3303e-05\n",
      "Epoch 336/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1411e-04 - val_loss: 1.1351e-04 - lr: 1.3303e-05\n",
      "Epoch 337/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1390e-04 - val_loss: 1.1378e-04 - lr: 1.3303e-05\n",
      "Epoch 338/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1373e-04 - val_loss: 1.1323e-04 - lr: 1.3303e-05\n",
      "Epoch 339/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1349e-04 - val_loss: 1.1353e-04 - lr: 1.3303e-05\n",
      "Epoch 340/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1270e-04 - val_loss: 1.1270e-04 - lr: 1.1973e-05\n",
      "Epoch 341/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1245e-04 - val_loss: 1.1239e-04 - lr: 1.1973e-05\n",
      "Epoch 342/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1226e-04 - val_loss: 1.1167e-04 - lr: 1.1973e-05\n",
      "Epoch 343/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1204e-04 - val_loss: 1.1191e-04 - lr: 1.1973e-05\n",
      "Epoch 344/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1191e-04 - val_loss: 1.1170e-04 - lr: 1.1973e-05\n",
      "Epoch 345/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1121e-04 - val_loss: 1.1111e-04 - lr: 1.0775e-05\n",
      "Epoch 346/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1089e-04 - val_loss: 1.1094e-04 - lr: 1.0775e-05\n",
      "Epoch 347/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1068e-04 - val_loss: 1.1085e-04 - lr: 1.0775e-05\n",
      "Epoch 348/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1051e-04 - val_loss: 1.1062e-04 - lr: 1.0775e-05\n",
      "Epoch 349/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1031e-04 - val_loss: 1.1015e-04 - lr: 1.0775e-05\n",
      "Epoch 350/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0965e-04 - val_loss: 1.1024e-04 - lr: 9.6977e-06\n",
      "Epoch 351/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0942e-04 - val_loss: 1.0975e-04 - lr: 9.6977e-06\n",
      "Epoch 352/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0927e-04 - val_loss: 1.0965e-04 - lr: 9.6977e-06\n",
      "Epoch 353/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0909e-04 - val_loss: 1.0963e-04 - lr: 9.6977e-06\n",
      "Epoch 354/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0900e-04 - val_loss: 1.0962e-04 - lr: 9.6977e-06\n",
      "Epoch 355/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0842e-04 - val_loss: 1.0869e-04 - lr: 8.7280e-06\n",
      "Epoch 356/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0818e-04 - val_loss: 1.0845e-04 - lr: 8.7280e-06\n",
      "Epoch 357/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0804e-04 - val_loss: 1.0860e-04 - lr: 8.7280e-06\n",
      "Epoch 358/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0783e-04 - val_loss: 1.0830e-04 - lr: 8.7280e-06\n",
      "Epoch 359/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0768e-04 - val_loss: 1.0858e-04 - lr: 8.7280e-06\n",
      "Epoch 360/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0718e-04 - val_loss: 1.0782e-04 - lr: 7.8552e-06\n",
      "Epoch 361/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0703e-04 - val_loss: 1.0808e-04 - lr: 7.8552e-06\n",
      "Epoch 362/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0687e-04 - val_loss: 1.0764e-04 - lr: 7.8552e-06\n",
      "Epoch 363/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0677e-04 - val_loss: 1.0738e-04 - lr: 7.8552e-06\n",
      "Epoch 364/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0659e-04 - val_loss: 1.0754e-04 - lr: 7.8552e-06\n",
      "Epoch 365/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0619e-04 - val_loss: 1.0692e-04 - lr: 7.0697e-06\n",
      "Epoch 366/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0600e-04 - val_loss: 1.0660e-04 - lr: 7.0697e-06\n",
      "Epoch 367/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0584e-04 - val_loss: 1.0658e-04 - lr: 7.0697e-06\n",
      "Epoch 368/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0574e-04 - val_loss: 1.0664e-04 - lr: 7.0697e-06\n",
      "Epoch 369/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0559e-04 - val_loss: 1.0626e-04 - lr: 7.0697e-06\n",
      "Epoch 370/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0517e-04 - val_loss: 1.0632e-04 - lr: 6.3627e-06\n",
      "Epoch 371/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0508e-04 - val_loss: 1.0582e-04 - lr: 6.3627e-06\n",
      "Epoch 372/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0492e-04 - val_loss: 1.0583e-04 - lr: 6.3627e-06\n",
      "Epoch 373/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0479e-04 - val_loss: 1.0585e-04 - lr: 6.3627e-06\n",
      "Epoch 374/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0471e-04 - val_loss: 1.0556e-04 - lr: 6.3627e-06\n",
      "Epoch 375/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0436e-04 - val_loss: 1.0517e-04 - lr: 5.7264e-06\n",
      "Epoch 376/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0420e-04 - val_loss: 1.0535e-04 - lr: 5.7264e-06\n",
      "Epoch 377/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0409e-04 - val_loss: 1.0514e-04 - lr: 5.7264e-06\n",
      "Epoch 378/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0399e-04 - val_loss: 1.0495e-04 - lr: 5.7264e-06\n",
      "Epoch 379/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0390e-04 - val_loss: 1.0487e-04 - lr: 5.7264e-06\n",
      "Epoch 380/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0357e-04 - val_loss: 1.0460e-04 - lr: 5.1538e-06\n",
      "Epoch 381/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0346e-04 - val_loss: 1.0439e-04 - lr: 5.1538e-06\n",
      "Epoch 382/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0336e-04 - val_loss: 1.0443e-04 - lr: 5.1538e-06\n",
      "Epoch 383/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0323e-04 - val_loss: 1.0433e-04 - lr: 5.1538e-06\n",
      "Epoch 384/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0314e-04 - val_loss: 1.0449e-04 - lr: 5.1538e-06\n",
      "Epoch 385/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0285e-04 - val_loss: 1.0399e-04 - lr: 4.6384e-06\n",
      "Epoch 386/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0274e-04 - val_loss: 1.0414e-04 - lr: 4.6384e-06\n",
      "Epoch 387/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0264e-04 - val_loss: 1.0381e-04 - lr: 4.6384e-06\n",
      "Epoch 388/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0256e-04 - val_loss: 1.0372e-04 - lr: 4.6384e-06\n",
      "Epoch 389/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0249e-04 - val_loss: 1.0379e-04 - lr: 4.6384e-06\n",
      "Epoch 390/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0222e-04 - val_loss: 1.0349e-04 - lr: 4.1746e-06\n",
      "Epoch 391/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0214e-04 - val_loss: 1.0362e-04 - lr: 4.1746e-06\n",
      "Epoch 392/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0206e-04 - val_loss: 1.0320e-04 - lr: 4.1746e-06\n",
      "Epoch 393/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0197e-04 - val_loss: 1.0318e-04 - lr: 4.1746e-06\n",
      "Epoch 394/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0188e-04 - val_loss: 1.0321e-04 - lr: 4.1746e-06\n",
      "Epoch 395/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0163e-04 - val_loss: 1.0291e-04 - lr: 3.7571e-06\n",
      "Epoch 396/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0156e-04 - val_loss: 1.0326e-04 - lr: 3.7571e-06\n",
      "Epoch 397/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0148e-04 - val_loss: 1.0292e-04 - lr: 3.7571e-06\n",
      "Epoch 398/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0142e-04 - val_loss: 1.0293e-04 - lr: 3.7571e-06\n",
      "Epoch 399/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0133e-04 - val_loss: 1.0278e-04 - lr: 3.7571e-06\n",
      "Epoch 400/400\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0114e-04 - val_loss: 1.0277e-04 - lr: 3.3814e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00bc4bc410>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=5)\n",
    "model.fit(   # 根据情况调整参数\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    epochs=400,\n",
    "    batch_size=128,\n",
    "    callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c4e5f76-cdd8-4d3f-a6da-32263c60d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 MSE:0.0001\n",
      "y1 MSE:2998.0000 ------ 2998\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_features)\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels, test_preds))\n",
    "print(\"y1 MSE:%.4f\" % len(test_labels), '------',len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f97739de-d01c-49fc-b8f3-b1b6ac40d4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /models/slot0/20230913165945/assets\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime                                                                                                                                                                \n",
    "\n",
    "model_version =  datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y%m%d%H%M%S')\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    f'/models/slot0/{model_version}/', # v1/models/slot0/为tensorflow-serving的模型根目录\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc678317-f4a5-49b2-85e4-7f25f0d95332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639, 12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
      "{'outputs': [[0.342613637, -0.931896627, 0.285090864, -0.150799677, -0.835590541, 0.771899, 0.461140096, 1.91012442]]}\n",
      "{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
      "y1 MSE:0.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "test_features=pd.DataFrame(test_features)\n",
    "test_labels=pd.DataFrame(test_labels)\n",
    "req_data = json.dumps({\n",
    "            'inputs': test_features.values[:1].tolist()\n",
    "        })  \n",
    "print(req_data)\n",
    "response = requests.post(f'http://fireeye-test-model-container:8501/v1/models/slot0/versions/{model_version}:predict', # 根据部署地址填写\n",
    "                         data=req_data,\n",
    "                         headers={\"content-type\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError('Request tf-serving failed: ' + response.text)\n",
    "resp_data = json.loads(response.text)    \n",
    "if 'outputs' not in resp_data \\\n",
    "                    or type(resp_data['outputs']) is not list:\n",
    "    raise ValueError('Malformed tf-serving response')\n",
    "\n",
    "print(resp_data)\n",
    "print(\"{'outputs':\",test_labels.values[:1].tolist())\n",
    "\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels.values[:1].tolist(), resp_data['outputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5011d7-455f-44c0-b2ed-e2e10bbb1101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
